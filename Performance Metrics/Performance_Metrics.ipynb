{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy = No. of correct predictions / Total no. of predictions<br>\n",
    "2. For binary classification accuracy can also be,<br>\n",
    "   (TP + TN) / (TP + TN + FP + FN)<br>\n",
    "       TP -> True Positive<br>\n",
    "       TN -> True Negative<br>\n",
    "       FP -> False Positive<br>\n",
    "       FN -> False Negative<br>\n",
    "2. Values between 0 and 1<br>\n",
    "3. It can be used for Classification model<br>\n",
    "4. Very easy to measure<br>\n",
    "5. Very good measure for balanced datastes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Disadvantages :</b><br>\n",
    "1. It is not good metric for imbalanced data. Majority class will become dominant.<br>\n",
    "2. Dumb model will give high accuracy for imbalanced data. (A model is said to be dumb if it predicts same class labels irrespective of what input is given.)<br>\n",
    "3. Never use Accuracy for imbalanced dataset<br>\n",
    "4. Accuracy doesnot use probability scores.<br>\n",
    "5. Accuracy for imbalanced datasets give biased results and overfit model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Confusion_Matrix.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is a matrix of Actual class label vs Predicted class label.<br>\n",
    "2. For binary classification it is 2×2 matrix and for multiclass classification m×m matrix.<br>\n",
    "3. For a good model all principal diagonal values should be high and all off diagonal values should be low.<br>\n",
    "4. Four important terms of confusion matrix,<br>\n",
    "       True Positives : Predicted is YES and actual is also YES<br>\n",
    "       True Negative : Predicted is NO and actual is also NO<br>\n",
    "       False Positive : Predicted is YES but actual is NO<br>\n",
    "       False Negative : Predicted is NO but actual is YES<br>\n",
    "5. Other important terms,<br>\n",
    "     TPR(True Positive Rate):<br>\n",
    "        TP(True Positive) / P(Total Posotive)\n",
    "     TNR(True Negative Rate):<br>\n",
    "        TN(True Negative) / N(Total Negative)\n",
    "     FPR(False Positive Rate):<br>\n",
    "        FP(False Positive) / N(Total Negative)\n",
    "     FNR(False Negative Rate):<br>\n",
    "        FN(False Negative) / P(Total Positive)\n",
    "6. Model is good if TPR and TNR is high; FPR and FNR is low.<br>\n",
    "7. Even for imbalanced datasets by seeing TPR, TNR, FPR, FNR we can conclude whether model is dumb or sensible.<br>\n",
    "8. The values of terms TPR, TNR, FPR and FNR; are domain specific.<br>\n",
    "   Ex:- Diagnose Cancer or not. TPR should be very high and FNR should be very low.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Disadvantages :</b><br>\n",
    "    1. Confusion matrix doesnot process probability scores.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Of all the points the model predicted to be positive, what percentage of them are actually positive.<br>\n",
    "2. Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.<br>\n",
    "3. Precision = TP / (TP + FP)<br>\n",
    "4. Precision value is between 0 and 1.<br>\n",
    "5. Of all the actully positive points, how many of them predicted to be positive.<br>\n",
    "5. Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive).<br>\n",
    "6. Recall = TP / (TP + FN)<br>\n",
    "7. Recall value is between 0 and 1.<br>\n",
    "8. Because of values between 0 and 1, interpretation is easy for both precision and recall than F1-score.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. F1 Score = 2 * ((Precision * Recall) / (Precision + Recall))<br>\n",
    "2. F1 Score is high if both Precision and Recall values are high.<br>\n",
    "3. F1 Score is not easy to interprit like precision and recall.<br>\n",
    "4. F1 Score is needed when you want to seek a balance between Precision and Recall.<br>\n",
    "5.  F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC curve and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AUC - ROC curve is a performance measurement for classification problem at various thresholds settings.<br>\n",
    "2. ROC is a probability curve and AUC represents degree or measure of separability.<br>\n",
    "3. It tells how much model is capable of distinguishing between classes.<br>\n",
    "4. ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.<br>\n",
    "5. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.<br>\n",
    "6. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.<br>\n",
    "7. AUC mainly used for binary classification.<br>\n",
    "8. Variant of AUC are used for multiclass classification.<br>\n",
    "9. ROC curve is FPR as x-axis vs TPR as y-axis.<br>\n",
    "10. AUC of random model is a straightline and value is exactly 0.5.<br>\n",
    "11. AUC > 0.5 is a good model and AUC < 0.5 is worst model.<br>\n",
    "12. If AUC is less than 0.5 then swap class labels.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](roc_auc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why need use ROC-AUC :</b><br>\n",
    "    1. If we have to trade off between two models confusion matrix won't help as there are 4 terms.<br>\n",
    "    2. If we use F1 Score there we use only precision and recall and not TPR, FPR.<br>\n",
    "    3. We can keep ROC-AUC as primary metric then we can go for F1-Score and confusion matrix.<br>\n",
    "    4. Log loss uses true probabilities and calculates how far is the predicted from actual and penalizes.<br>\n",
    "    5. Log loss also doesn't use TPR and FPR, which also helps.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Disadvantages :</b><br>\n",
    "    1. AUC can be high for dumb model/very simple model.<br>\n",
    "    2. AUC doesn't care about actual scores but cares about ordering as we sort each iteration.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross Entropy / Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Log loss uses actual probability scores.<br>\n",
    "2. Log Loss is the most important classification metric based on probabilities.<br>\n",
    "3. It is negative of average of sum of log probability correct class label.<br>\n",
    "4. If probability value is high then loss is minimal. If probability values is low then loss is maximum.<br>\n",
    "5. This metric penalizes deviation from actual probability score.<br>\n",
    "6. Good model has smaller log-loss values.<br>\n",
    "7. Log-loss values is 0 to ∞(infinity).<br>\n",
    "8. Log-loss is hard to interpret.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Log-Loss : ![title](log_loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Log-Loss : ![title](multi_log_loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. R-Squared / Coefficient of Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is sum of squares of errors using mean.<br>\n",
    "2. It is used for regression models.<br>\n",
    "3. If R² is zero then the model is same as simple mean modle.<br>\n",
    "4. If R² is one then model is the best.<br>\n",
    "5. If R² is negative then model is worst model.<br>\n",
    "6. R² is not very robust to outliers.<br>\n",
    "7. If any one error is large then overall value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squared error : ![title](r_squared.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Disadvantages :</b>\n",
    "    1. R² is not very robust to outliers.\n",
    "    2. If any one error is large then overall value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Median Absolute Deviation(MAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:<br>\n",
    "    1. Find the median. The median for this set of numbers is 8.<br>\n",
    "    2. Subtract the median from each x-value using the formula |yi – median|.<br>\n",
    "       |3 – 8| = 5<br>\n",
    "       |8 – 8| = 0<br>\n",
    "       |8 – 8| = 0<br>\n",
    "       |8 – 8| = 0<br>\n",
    "       |8 – 8| = 0<br>\n",
    "       |9 – 8| = 1<br>\n",
    "       |9 – 8| = 1<br>\n",
    "       |9 – 8| = 1<br>\n",
    "       |9 – 8| = 1<br>\n",
    "    3. Find the median of the absolute differences. The median of the differences (0,0,0,0,1,1,1,1,5) is 1.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is robust for the outliers.<br>\n",
    "2. If the data is not normal distributes then this is suitable.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAD = median(|Yi – median(Yi|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distribution of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using errors plotting PDF and CDF, we can compare two models.<br>\n",
    "2. Ideal distribution of errors is most of the values are zero i.e. peak at 0 in pdf.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reference links:</b><br>\n",
    "    1. Confusion matrix Plotting: \n",
    "       https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
    "    2. Regression measures :\n",
    "       https://stats.stackexchange.com/questions/142873/how-to-determine-the-accuracy-of-regression-which-measure-should-be-used\n",
    "    3. Precision, recall and f1 score:\n",
    "       https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "    4. AUC ROC curve :\n",
    "       https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "    5. Why to use ROC-AUC:\n",
    "       https://soundcloud.com/applied-ai-course/why-auc\n",
    "    6. Log loss :\n",
    "       https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "    7. Why log-loss:\n",
    "       https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d\n",
    "    8. Median Absolute Deviation(MAD):\n",
    "       https://www.statisticshowto.com/median-absolute-deviation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
