{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenizing text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# paragraph of text\n",
    "text = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the\n",
    "nltk.tokenize.punkt module. This instance has already been trained and works well for\n",
    "many European languages. So it knows what punctuation and characters mark the end of a\n",
    "sentence and the beginning of a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance used in sent_tokenize() is actually loaded on demand from a pickle\n",
    "file. So if you're going to be tokenizing a lot of sentences, it's more efficient to load the\n",
    "PunktSentenceTokenizer class once, and call its tokenize() method instead.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "# paragraph of text\n",
    "text = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "Note :<br>\n",
    "    1> For other languages<br>\n",
    "       >>> spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')<br>\n",
    "       >>> spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')<br>\n",
    "           ['Hola amigo.', 'Estoy bien.']<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizing Sentences into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize('Hi how are you!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "The word_tokenize() function is a wrapper function that calls tokenize() on an\n",
    "instance of the TreebankWordTokenizer class. It's equivalent to the following code:\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('Hi how are you!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenizing Sentences Using Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can be used if you want complete control over how to tokenize text.<br>\n",
    "\n",
    "First you need to decide how you want to tokenize a piece of text as this will determine how<br>\n",
    "you construct your regular expression. The choices are:<br>\n",
    "1> Match on the tokens<br>\n",
    "2> Match on the separators or gaps<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'be', 'done', 'this']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(\"Can't be done this.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'be', 'done', 'this.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(\"Can't be done this.\", \"\\s+\", gaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's default sentence tokenizer is general purpose, and usually works quite well. But\n",
    "sometimes it is not the best choice for your text. Perhaps our text uses nonstandard\n",
    "punctuation, or is formatted in a unique way. In such cases, training your own sentence\n",
    "tokenizer can result in much more accurate sentence tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "\n",
      "\n",
      "Girl: But you already have a Big Mac...\n"
     ]
    }
   ],
   "source": [
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "print(sents1[0])\n",
    "print('\\n')\n",
    "print(sents1[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "\n",
      "\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the results to the default sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(text)\n",
    "print(sents2[0])\n",
    "print('\\n')\n",
    "print(sents2[678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "While the first sentence is the same, we can see that the tokenizers disagree on how to\n",
    "tokenize sentence 679 (this is the first sentence where the tokenizers diverge). The default\n",
    "tokenizer includes the next line of dialog, while our custom tokenizer correctly thinks that\n",
    "the next line is a separate sentence. This difference is a good demonstration of why it can\n",
    "be useful to train your own sentence tokenizer, especially when your text isn't in the typical\n",
    "paragraph-sentence structure\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Filtering Stopwords in a Tokenized Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words that generally do not contribute to the meaning of a sentence,\n",
    "at least for the purposes of information retrieval and natural language processing. These are\n",
    "words such as the and a. Most search engines will filter out stopwords from search queries\n",
    "and documents in order to save space in their index.<br>\n",
    "\n",
    "NLTK comes with a stopwords corpus that contains word lists for many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'done']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of all English stopwords, then use it to filter stopwords from a sentence\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'be', 'done', 'this']\n",
    "[word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are also stopword lists for many other languages\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'en', 'van', 'ik', 'te']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 stopwords of dutch language\n",
    "stopwords.words('dutch')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Looking Up Synsets(synonims) for a Word in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a lexical database for the English language. In other words, it's a dictionary\n",
    "designed specifically for natural language processing.<br>\n",
    "\n",
    "NLTK comes with a simple interface to look up words in WordNet. What you get is a list of\n",
    "Synset instances, which are groupings of synonymous words that express the same concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book.n.01\n",
      "\n",
      "\n",
      "Definition :\n",
      " a written work or composition that has been published (printed on pages bound together)\n",
      "\n",
      "\n",
      "Example : \n",
      " ['I am reading a good book on economics']\n",
      "\n",
      "\n",
      "parts of speech :\n",
      " n\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('book')[0]\n",
    "print(syn.name())\n",
    "print('\\n')\n",
    "print('Definition :\\n',syn.definition())\n",
    "print('\\n')\n",
    "print('Example : \\n',syn.examples())\n",
    "print('\\n')\n",
    "print('parts of speech :\\n', syn.pos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Looking up Lemmas and Synonyms in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemma (in linguistics), is the canonical form or morphological form of a word.<br>\n",
    "\n",
    "we can also look up lemmas in WordNet to find synonyms of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('cookbook.n.01.cookbook'), Lemma('cookbook.n.01.cookery_book')]\n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "print(lemmas)\n",
    "print('\\n')\n",
    "print(len(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook\n",
      "cookery_book\n"
     ]
    }
   ],
   "source": [
    "print(lemmas[0].name())\n",
    "print(lemmas[1].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].synset() == lemmas[1].synset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stemming Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.<br>\n",
    "\n",
    "Stemming is most commonly used by search engines for indexing\n",
    "words. Instead of storing all forms of a word, a search engine can store only the stems, greatly\n",
    "reducing the size of index while increasing retrieval accuracy.<br>\n",
    "\n",
    "One of the most common stemming algorithms is the Porter stemming algorithm. It is designed to remove and replace well-known suffixes of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookeri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'))\n",
    "print(stemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Lemmatizing Words with WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is very similar to stemming, but is more akin to synonym replacement.\n",
    "A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always\n",
    "left with a valid word that means the same thing.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "# We will use the WordNetLemmatizer class to find lemmas\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer =  WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Replacing Words Matching Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a number of replacement patterns. This will be a list of tuple\n",
    "pairs, where the first element is the pattern to match with and the second element is\n",
    "the replacement.<br>\n",
    "\n",
    "Next, we will create a RegexpReplacer class that will compile the patterns and provide\n",
    "a replace() method to substitute all the found patterns with their replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self. patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cannot is a contraction'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer = RegexReplacer()\n",
    "replacer.replace(\"can't is a contraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I should have done that thing I did not do'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"I should've done that thing I didn't do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
