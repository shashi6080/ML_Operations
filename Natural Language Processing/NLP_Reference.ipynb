{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenizing text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# paragraph of text\n",
    "text = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "The sent_tokenize function uses an instance of PunktSentenceTokenizer from the\n",
    "nltk.tokenize.punkt module. This instance has already been trained and works well for\n",
    "many European languages. So it knows what punctuation and characters mark the end of a\n",
    "sentence and the beginning of a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance used in sent_tokenize() is actually loaded on demand from a pickle\n",
    "file. So if you're going to be tokenizing a lot of sentences, it's more efficient to load the\n",
    "PunktSentenceTokenizer class once, and call its tokenize() method instead.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "\n",
    "# paragraph of text\n",
    "text = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "Note :<br>\n",
    "    1> For other languages<br>\n",
    "       >>> spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')<br>\n",
    "       >>> spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')<br>\n",
    "           ['Hola amigo.', 'Estoy bien.']<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizing Sentences into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize('Hi how are you!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "The word_tokenize() function is a wrapper function that calls tokenize() on an\n",
    "instance of the TreebankWordTokenizer class. It's equivalent to the following code:\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('Hi how are you!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenizing Sentences Using Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can be used if you want complete control over how to tokenize text.<br>\n",
    "\n",
    "First you need to decide how you want to tokenize a piece of text as this will determine how<br>\n",
    "you construct your regular expression. The choices are:<br>\n",
    "1> Match on the tokens<br>\n",
    "2> Match on the separators or gaps<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'be', 'done', 'this']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(\"Can't be done this.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'be', 'done', 'this.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(\"Can't be done this.\", \"\\s+\", gaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's default sentence tokenizer is general purpose, and usually works quite well. But\n",
    "sometimes it is not the best choice for your text. Perhaps our text uses nonstandard\n",
    "punctuation, or is formatted in a unique way. In such cases, training your own sentence\n",
    "tokenizer can result in much more accurate sentence tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "\n",
      "\n",
      "Girl: But you already have a Big Mac...\n"
     ]
    }
   ],
   "source": [
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "print(sents1[0])\n",
    "print('\\n')\n",
    "print(sents1[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White guy: So, do you have any plans for this evening?\n",
      "\n",
      "\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the results to the default sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sents2 = sent_tokenize(text)\n",
    "print(sents2[0])\n",
    "print('\\n')\n",
    "print(sents2[678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "While the first sentence is the same, we can see that the tokenizers disagree on how to\n",
    "tokenize sentence 679 (this is the first sentence where the tokenizers diverge). The default\n",
    "tokenizer includes the next line of dialog, while our custom tokenizer correctly thinks that\n",
    "the next line is a separate sentence. This difference is a good demonstration of why it can\n",
    "be useful to train your own sentence tokenizer, especially when your text isn't in the typical\n",
    "paragraph-sentence structure\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Filtering Stopwords in a Tokenized Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words that generally do not contribute to the meaning of a sentence,\n",
    "at least for the purposes of information retrieval and natural language processing. These are\n",
    "words such as the and a. Most search engines will filter out stopwords from search queries\n",
    "and documents in order to save space in their index.<br>\n",
    "\n",
    "NLTK comes with a stopwords corpus that contains word lists for many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'done']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of all English stopwords, then use it to filter stopwords from a sentence\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'be', 'done', 'this']\n",
    "[word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are also stopword lists for many other languages\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'en', 'van', 'ik', 'te']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 stopwords of dutch language\n",
    "stopwords.words('dutch')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Looking Up Synsets(synonims) for a Word in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a lexical database for the English language. In other words, it's a dictionary\n",
    "designed specifically for natural language processing.<br>\n",
    "\n",
    "NLTK comes with a simple interface to look up words in WordNet. What you get is a list of\n",
    "Synset instances, which are groupings of synonymous words that express the same concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book.n.01\n",
      "\n",
      "\n",
      "Definition :\n",
      " a written work or composition that has been published (printed on pages bound together)\n",
      "\n",
      "\n",
      "Example : \n",
      " ['I am reading a good book on economics']\n",
      "\n",
      "\n",
      "parts of speech :\n",
      " n\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('book')[0]\n",
    "print(syn.name())\n",
    "print('\\n')\n",
    "print('Definition :\\n',syn.definition())\n",
    "print('\\n')\n",
    "print('Example : \\n',syn.examples())\n",
    "print('\\n')\n",
    "print('parts of speech :\\n', syn.pos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Looking up Lemmas and Synonyms in WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemma (in linguistics), is the canonical form or morphological form of a word.<br>\n",
    "\n",
    "we can also look up lemmas in WordNet to find synonyms of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('cookbook.n.01.cookbook'), Lemma('cookbook.n.01.cookery_book')]\n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "print(lemmas)\n",
    "print('\\n')\n",
    "print(len(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook\n",
      "cookery_book\n"
     ]
    }
   ],
   "source": [
    "print(lemmas[0].name())\n",
    "print(lemmas[1].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].synset() == lemmas[1].synset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Stemming Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a technique to remove affixes from a word, ending up with the stem. For\n",
    "example, the stem of cooking is cook, and a good stemming algorithm knows that the ing\n",
    "suffix can be removed.<br>\n",
    "\n",
    "Stemming is most commonly used by search engines for indexing\n",
    "words. Instead of storing all forms of a word, a search engine can store only the stems, greatly\n",
    "reducing the size of index while increasing retrieval accuracy.<br>\n",
    "\n",
    "One of the most common stemming algorithms is the Porter stemming algorithm. It is designed to remove and replace well-known suffixes of English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookeri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'))\n",
    "print(stemmer.stem('cookery'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Lemmatizing Words with WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is very similar to stemming, but is more akin to synonym replacement.\n",
    "A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always\n",
    "left with a valid word that means the same thing.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "# We will use the WordNetLemmatizer class to find lemmas\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer =  WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Replacing Words Matching Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a number of replacement patterns. This will be a list of tuple\n",
    "pairs, where the first element is the pattern to match with and the second element is\n",
    "the replacement.<br>\n",
    "\n",
    "Next, we will create a RegexpReplacer class that will compile the patterns and provide\n",
    "a replace() method to substitute all the found patterns with their replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "(r'won\\'t', 'will not'),\n",
    "(r'can\\'t', 'cannot'),\n",
    "(r'i\\'m', 'i am'),\n",
    "(r'ain\\'t', 'is not'),\n",
    "(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "(r'(\\w+)\\'d', '\\g<1> would')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for (pattern, repl) in self. patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cannot is a contraction'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer = RegexReplacer()\n",
    "replacer.replace(\"can't is a contraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I should have done that thing I did not do'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"I should've done that thing I didn't do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. NLP Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing :</b>\n",
    "* Removing special characters\n",
    "* Removing numbers\n",
    "* Take care of case letters\n",
    "* Abbrevations\n",
    "* Standardization of words\n",
    "* Removing sparse words\n",
    "* Lemmatization\n",
    "* Stemming\n",
    "* Spelling corrections\n",
    "* Handling other symbols (emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Converting text to numeric vector</b>\n",
    "* Count\n",
    "* TF-IDF\n",
    "* Hash\n",
    "* Word Embeddings (word2vec, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Features</b>\n",
    "* Words\n",
    "* N-grams\n",
    "* Characters\n",
    "* Derived features(no of letters, no of sentences, no of caps/small, no of POS related like noun/verb/adjuctive etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exploratory Analysis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word associations\n",
    "* Word counts(word cloud, bar charts)\n",
    "* Similarity between documents, words/associations\n",
    "* TOPIC mining / clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Text mining applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Many to one :</b> Sentiment analysis, text recommendation<br>\n",
    "<b>One to many :</b> Image caption(series of words)<br>\n",
    "<b>Many to many :</b> Chatbot(conversation), Machine translations<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. NLP, NLU and NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NLP :</b> Processing text<br>\n",
    "<b>NLU :</b> Exploratory analysis, predictive analysis<br>\n",
    "<b>NLG :</b> Text generation (chatbot, voicebot, machine translations, text recommandation, caption generation, subtiles generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technique to turn words into numbers\n",
    "* Will take care of relationship between the words\n",
    "* Context based vectorization\n",
    "* Word embedding is learnt from the data\n",
    "* It is dense as compared to normal one hot encoding\n",
    "* Idea is to embed words into lower dimensional space\n",
    "* Dimensions of this spaceare typically defined by word context, i.e. semantically similar wordsare embedde near each other\n",
    "* Popular algorithms are : Word2Vec (skipgram, CBOW), Glove, FastTect, PMI\n",
    "* We can use pre-trained models of Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How to implement?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pre-trained models : Google news articles, wikipedia articles, american news agency, routers\n",
    "2. Build it from scratch : Using Gensim\n",
    "3. While building deep learning model use embedded layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Bag of Words(BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r1 : This pasta is very tasty and affordable<br>\n",
    "r2 : This pasta is not tasty and is affordable<br>\n",
    "r3 : This pasta is delicious and cheap<br>\n",
    "r4 : Pasta is tasty and pasts tastes good<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r1 -> Review / Text document<br>\n",
    "n  -> No. of reviews / documents \n",
    "corpus -> Collection of all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Constucting a set/dictionary of all words in all reviews\n",
    "2. For each review create vector.\n",
    "3. Vector consisting of each word as each document\n",
    "4. Each word is a different dimension\n",
    "5. Each cell in vector is no. of times that text occurs in that review\n",
    "6. Dimesions of the vector is very large\n",
    "7. Vector is sparse vector. Most of the elements of the vector are zero.\n",
    "8. Similar documents have closer distance\n",
    "9. BOW is counting common words\n",
    "10. It won't consider symantic meaning os words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Binary Bag of Words</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vector consists of boolean value of occurance of word instead count\n",
    "* It is also called boolean Bag of Words\n",
    "* Value is '1' if atleast once the word occurs in document\n",
    "* Value is '0' if the word doesn't occur in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "final_counts = count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\n",
    "print(\"the number of unique words \", final_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing few words for the document doesn't affect meaning. Such words are stopwords\n",
    "* Ex :- the, this, and, is, at etc.\n",
    "* By removing stop words, BOW will be smaller ans more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english')) #set of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can edit this set of stopwords according to out need. Ex:- removing 'not' from the set for the problem of review polarity.\n",
    "* Carefully we need to remove stopwords. We should not lose information by removing stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
    "* Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n",
    "* This indiscriminate cutting can be successful in some occasions, but not always\n",
    "* Ex :-<br>\n",
    "<b>|FORM|SUFFIX|STEM|</b><br>\n",
    "|studies|-es|studi|<br>\n",
    "|studying|-ing|study|<br>\n",
    "* There are different algorithms that can be used in the stemming process, but the most common in English is Porter stemmer.\n",
    "* Stemming is definitely the simpler of the two approaches. \n",
    "* With stemming, words are reduced to their word stems.\n",
    "* Different algorithms are Porter stemmer, Snowball stemmer, Lancaster stemmer\n",
    "* Search engines use stemming for indexing the words. That’s why rather than storing all forms of a word, a search engine can store only the stems. In this way, stemming reduces the size of the index and increases retrieval accuracy.\n",
    "* Stemming is the process of converting the words of a sentence to its non-changing portions. In the example of amusing, amusement, and amused above, the stem would be amus.\n",
    "* Lemmatization is the process of converting the words of a sentence to its dictionary form. For example, given the words amusement, amusing, and amused, the lemma for each and all would be amuse.\n",
    "* Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Porter Stemmer :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the stemmer recognizes “runner” as a noun, not a verb form or participle. Also, the adverbs “easily” and “fairly” are stemmed to the unusual root “easili” and “fairli”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Snowball Stemmer :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lancaster Stemmer :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "Lanc_stemmer.stem('eats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Takes into consideration the morphological analysis of the words\n",
    "* lemmatization looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words.\n",
    "* To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma.\n",
    "* Ex:-<br>\n",
    "<b>|FORM|LEMMA|</b><br>\n",
    "|studies|study|<br>\n",
    "|studying|study|<br>\n",
    "* To extract the proper lemma, it is necessary to look at the morphological analysis of each word. This requires having dictionaries for every language to provide that kind of analysis.\n",
    "* The lemma of ‘was’ is ‘be’ and the lemma of ‘mice’ is ‘mouse’.\n",
    "* Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of Stemming\n",
    "* Lemmatization looks at surrounding text to determine a given word’s part of speech, it does not categorize phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "I            PRON   561228191312463089     -PRON-\n",
    "saw          VERB   11925638236994514241   see\n",
    "eighteen     NUM    9609336664675087640    eighteen\n",
    "mice         NOUN   1384165645700560590    mouse\n",
    "today        NOUN   11042482332948150395   today\n",
    "!            PUNCT  17494803046312582752   !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Conclusion Stemming vs Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One thing to note about lemmatization is that it is harder to create a lemmatizer in a new language than it is a stemming algorithm because we require a lot more knowledge about structure of a language in lemmatizers.\n",
    "* Stemming and Lemmatization both generate the foundation sort of the inflected words and therefore the only difference is that stem may not be an actual word whereas, lemma is an actual language word.\n",
    "* Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used a corpus also to supply lemma which makes it slower than stemming. you furthermore might had to define a parts-of-speech to get the proper lemma.\n",
    "* The above points show that if speed is concentrated then stemming should be used since lemmatizers scan a corpus which consumes time and processing. It depends on the problem you’re working on that decides if stemmers should be used or lemmatizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Uni gram, Bi gram and n gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Uni gram :</b> We use single word for each dimension.\n",
    "* <b>Bi gram :</b> We use 2 words for each dimension\n",
    "* <b>n gram :</b> We use 'n' words for each dimension\n",
    "* Uni grams ignore sequence information\n",
    "* Bi grams and n grams, keep some sequence information though not completely\n",
    "* No. of tri grams >= no. of bi grams >= no. of uni grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example : -</b><br>\n",
    "Sentence - \"I have a lovely dog\"<br>\n",
    "Uni grams - \"I\", \"have\", \"a\" , \"lovely\" , \"dog\"<br>\n",
    "Bi grams - \"I have\" , \"have a\" , \"a lovely\" , \"lovely dog\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2) ) #in scikit-learn\n",
    "final_bigram_counts = count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Term Frequency Inverse Document Frequency\n",
    "* TF(wi, rj) = # of times wi occur in rj / total # of words in rj\n",
    "* 0 <= TF(wi, rj) <= 1\n",
    "* TF is probability of finding word in document\n",
    "* TF is how ofter wi occur in rj\n",
    "* IDF(wi, Dc) = log(# of documents / # of docs containing wi) = log(N/ni)\n",
    "* IDF(wi, Dc) >= 0\n",
    "* If Wi is more common in corpus then IDF value is lower\n",
    "* If Wi is less common in corpus then IDF value is higher\n",
    "* While converting text to numeric, in the vector we add TF\\*IDF value of text in place of word\n",
    "* TF-IDF gives more importance to rarer words in the corpus and words which are frequent in documents\n",
    "* <b>TF-IDF doesnot donsider symantic meaning of the text</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r1 : w1, w2, w3, w2, w5<br>\n",
    "r2 : w1, w3, w4, w5, w6, w2<br>\n",
    "r3 : ...............<br>\n",
    "r4 : ...............<br>\n",
    "Dc = {r1, r2, r3.....}<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['CleanedText'].values)\n",
    "print(\"the type of count vectorizer \",type(final_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "print(\"some sample features(unique words in the corpus)\",features[100000:100010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2vec is a group of related models that are used to produce word embeddings.\n",
    "* These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\n",
    "* Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. \n",
    "* Each word is represented as dense vectors, with typically with 50, 100, 200, 300 dimensions \n",
    "*  Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space\n",
    "* Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network.\n",
    "* Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW)\n",
    "* Skip Gram works well with small amount of data and is found to represent rare words well.\n",
    "* On the other hand, CBOW is faster and has better representations for more frequent words.\n",
    "* Google has already trained word2vec, and has 300 dimensions\n",
    "* Word2vec considers neighbourhood of word during training. If neighbourhoods are same then vectors should be close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Average Word2Vec :</b><br>\n",
    "If r1 : w1, w2, w3, w4, w5<br>\n",
    "v1 = 1/n*(w2v(w1)+w2v(w2)+w2v(w3)+w2v(w4)+w2v(w5))<br>\n",
    "<br>\n",
    "It works well for sometimes as this is simple method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TF-IDF weighted Word2vec :</b><br>\n",
    "If r1 : w1, w2, w3, w4, w5<br>\n",
    "v1 = ((t1\\*w2v(w1))+(t2\\*w2v(w2))+(t3\\*w2v(w3))+(t4\\*w2v(w4))+(t5\\*w2v(w5))) / (t1+t2+t3+t4+t5)<br>\n",
    "v1 = Sigma(ti\\*w2v(wi))/Sigma(ti)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
    "vector = model.wv['computer']  # numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Google News Word2Vectors\n",
    "\n",
    "# in this project we are using a pretrained model by google\n",
    "# its 3.3G file, once you load this into your memory \n",
    "# it occupies ~9Gb, so please do this step only if you have >12G of ram\n",
    "# we will provide a pickle file wich contains a dict , \n",
    "# and it contains all our courpus words as keys and  model[word] as values\n",
    "# To use this code-snippet, download \"GoogleNews-vectors-negative300.bin\" \n",
    "# from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "# it's 1.9GB in size.\n",
    "\n",
    "\n",
    "# http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W17SRFAzZPY\n",
    "# you can comment this whole cell\n",
    "# or change these varible according to your need\n",
    "is_your_ram_gt_16g=False\n",
    "want_to_read_sub_set_of_google_w2v = True\n",
    "want_to_read_whole_google_w2v = True\n",
    "if not is_your_ram_gt_16g:\n",
    "    if want_to_read_sub_set_of_google_w2v and  os.path.isfile('google_w2v_for_amazon.pkl'):\n",
    "        with open('google_w2v_for_amazon.pkl', 'rb') as f:\n",
    "            # model is dict object, you can directly access any word vector using model[word]\n",
    "            model = pickle.load(f)\n",
    "else:\n",
    "    if want_to_read_whole_google_w2v and os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# print(\"the vector representation of word 'computer'\",model.wv['computer'])\n",
    "# print(\"the similarity between the words 'woman' and 'man'\",model.wv.similarity('woman', 'man'))\n",
    "# print(\"the most similar words to the word 'woman'\",model.wv.most_similar('woman'))\n",
    "# this will raise an error\n",
    "# model.wv.most_similar('tasti')  # \"tasti\" is the stemmed word for tasty, tastful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>REFERENCE :</b>\n",
    "1. Stemming and lemmatization : https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
    "2. Stemming and Lemmatization : https://towardsdatascience.com/stemming-vs-lemmatization-2daddabcb221\n",
    "3. Stemming and Lemmatization : https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    "4. Word Embeddings and Word2Vec : https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "5. Word embeddings and word2vec : https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "6. word2vec : https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
