{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression can be interpreter by geometry, probability and loss function\n",
    "* Model separates classes using line/plane/hyperplane\n",
    "* Model assumes data is linearly separable or almost linearly separable\n",
    "* Equation of a line is y = mx+c, wT+b=0, where 'w' is normal to the plane and 'b' is intercept term\n",
    "* If equation of the plane plasses through origin then wT=0 as b=0\n",
    "* Given Dn={+ve, -ve}, need to find 'w' and 'b'\n",
    "* Here task is to find the line/plane/hyperplane that best separaed between different classes\n",
    "* Optimization task is maximize(w)Σ(Yi\\*WTxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sigmoid or Squashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimization task maximize(w)Σ(Yi\\*WTxi), is prone to outliers. Need to change this to handle outliers.\n",
    "* Squashing is a technique to keep small value as it is and large value to squash it to small value.\n",
    "* We need to use extra function on top of optimization such function is Sigmoid funtion.\n",
    "* Sigmoid function, σ(x) = 1 / 1 + e**-x\n",
    "* Sigmoid max value is '1' and min value is '0'\n",
    "* σ(0)=0.5\n",
    "* There are several functions to choose for the same. But Sigmoid has probabilistic interpretation\n",
    "* Final optimization problem is, maximize(w)Σσ(Yi\\*WTxi)\n",
    "* Log(x) is monotonic function then final optimization function is, maximize(w)Σlog(σ(Yi\\*WTxi))\n",
    "* w* = argmin(w)Σlog(1+exp(-yi\\*wT\\*xi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimal 'w' is a weight vector\n",
    "* 'w' is D dimensional vector.\n",
    "* w = <w1, w2, w3, w4, .....wn>\n",
    "* These are weights associated with each feature <f1, f2, f3, f4,.....fn>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* w* = argmin(w)Σlog(1+exp(-yi\\*wT\\*xi)) + λ\\*wT\\*w\n",
    "* w* = Logistic loss + L2 regularizer\n",
    "* It is for controling overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. L1 Regularization for Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* w* = argmin(w)Σlog(1+exp(-yi\\*wT\\*xi)) + λ*|w|\n",
    "* If we use L1 regularizer in Logistic regression all less important features become zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* w = argmin(w)Σlog(1+exp(-yi\\wT*xi)) + λ1*wT*w + λ2*|w|\n",
    "* Here no of hyperparamers tuning increases, λ1, λ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
