{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Rescaling Numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale numerical data data to be between teo values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-500.5],\n",
       "       [-100.1],\n",
       "       [   0. ],\n",
       "       [ 100.1],\n",
       "       [ 900.9]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "feature = np.array([[-500.5], [-100.1], [0], [100.1], [900.9]])\n",
    "\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28571429],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_feature = minmax_scale.fit_transform(feature)\n",
    "\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Standardize a feature(Numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a feature to have mean 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-900.5],\n",
       "       [-250.5],\n",
       "       [ 150.9],\n",
       "       [ 711.9],\n",
       "       [9999.1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "feature = np.array([[-900.5], [-250.5], [150.9], [711.9], [9999.1]])\n",
    "\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69971382],\n",
       "       [-0.53971903],\n",
       "       [-0.4409161 ],\n",
       "       [-0.30282829],\n",
       "       [ 1.98317724]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "scaled_feature = standard_scaler.fit_transform(feature)\n",
    "\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalizing observations(Numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale the feature values of observations to have unit norm (total length of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4 ,  0.4 ],\n",
       "       [ 1.5 ,  3.5 ],\n",
       "       [ 1.2 , 15.5 ],\n",
       "       [ 1.89, 38.9 ],\n",
       "       [20.2 ,  5.2 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "feature = np.array([[0.4, 0.4], [1.5, 3.5], [1.2, 15.5], [1.89, 38.9], [20.2, 5.2]])\n",
    "\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.3939193 , 0.91914503],\n",
       "       [0.07718838, 0.99701653],\n",
       "       [0.04852887, 0.99882178],\n",
       "       [0.96842682, 0.24929799]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 normalizing\n",
    "normalizer = preprocessing.Normalizer(norm='l2')\n",
    "\n",
    "normalized_feature = normalizer.transform(feature)\n",
    "\n",
    "normalized_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.3       , 0.7       ],\n",
       "       [0.07185629, 0.92814371],\n",
       "       [0.04633489, 0.95366511],\n",
       "       [0.79527559, 0.20472441]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 normalizing\n",
    "normalizer = preprocessing.Normalizer(norm='l1')\n",
    "\n",
    "normalized_feature = normalizer.transform(feature)\n",
    "\n",
    "normalized_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Polynomial and Intercation features(Numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features are often created when we want to include the notion that there exists a nonlinear relationship between the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "features = np.array([[2, 3], [2, 3]])\n",
    "\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "transformed_features = polynomial_interaction.fit_transform(features)\n",
    "\n",
    "transformed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 6.],\n",
       "       [2., 3., 6.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting interaction to True\n",
    "\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "transformed_features = polynomial_interaction.fit_transform(features)\n",
    "\n",
    "transformed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom transforming features(Numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom transformation to one or more features. We might want to create a feature that is the natural log of values of different feature. We can do this by creating a function and then mapping it to features using sklearn FunctionTransformer or pandas apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 3],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "features = np.array([[2, 3], [2, 3], [2, 3]])\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_ten(x):\n",
    "    return x+10\n",
    "\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "\n",
    "transformed_features = ten_transformer.transform(features)\n",
    "\n",
    "transformed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature1  feature2\n",
      "0         2         3\n",
      "1         2         3\n",
      "2         2         3\n",
      "\n",
      "\n",
      "Converted back to numpy array :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 3],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Alternate way using pandas\n",
    "df = pd.DataFrame(features, columns=['feature1', 'feature2'])\n",
    "\n",
    "df.apply(add_ten)\n",
    "\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# convert back to numpy array\n",
    "print('Converted back to numpy array :')\n",
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Handling Outliers(Numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways we can handle outliers.\n",
    "1. We can drop them\n",
    "2. We can include them as feature\n",
    "3. We can transform them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Rooms  Square_Feet\n",
       "0   534433    2.0         1500\n",
       "1   392333    3.5         2500\n",
       "2   293222    2.0         1500\n",
       "3  4322032  116.0        40000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a dataframe\n",
    "houses = pd.DataFrame()\n",
    "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
    "houses['Rooms'] = [2, 3.5, 2, 116]\n",
    "houses['Square_Feet'] = [1500, 2500, 1500, 40000]\n",
    "\n",
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price  Rooms  Square_Feet\n",
       "0  534433    2.0         1500\n",
       "1  392333    3.5         2500\n",
       "2  293222    2.0         1500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First way, dropping the observation\n",
    "houses = houses[houses['Rooms'] < 20]\n",
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Rooms  Square_Feet  Outlier\n",
       "0   534433    2.0         1500        0\n",
       "1   392333    3.5         2500        0\n",
       "2   293222    2.0         1500        0\n",
       "3  4322032  116.0        40000        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note : execute first step before proceeding\n",
    "import numpy as np\n",
    "\n",
    "# Second way, incluse them as feature\n",
    "houses['Outlier'] = np.where(houses['Rooms'] < 20, 0, 1)\n",
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Log_of_square_feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>40000</td>\n",
       "      <td>10.596635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Rooms  Square_Feet  Log_of_square_feet\n",
       "0   534433    2.0         1500            7.313220\n",
       "1   392333    3.5         2500            7.824046\n",
       "2   293222    2.0         1500            7.313220\n",
       "3  4322032  116.0        40000           10.596635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note : execute first step before proceeding\n",
    "import numpy as np\n",
    "\n",
    "# Third way, transforming the features\n",
    "houses['Log_of_square_feet'] = [np.log(x) for x in houses['Square_Feet']]\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Discretizing Features(NUmerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have numerical values and want to break them into discrete bins.<br>\n",
    "Depending on how we want to break up data, there are two techniques.<br>\n",
    "-> Binarize feature according to some threshold<br>\n",
    "-> Break up numerical values according to multiple thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize feature according to some threshold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# feature\n",
    "age = np.array([[6], [12], [20], [36], [65]])\n",
    "\n",
    "binarizer = Binarizer(18)\n",
    "\n",
    "transformed_age = binarizer.fit_transform(age)\n",
    "transformed_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Break up numerical values according to multiple thresholds\n",
    "\n",
    "second_transformed_age = np.digitize(age, bins=[20, 30, 40]) # right=True, includes bins as well\n",
    "second_transformed_age\n",
    "\n",
    "# Note : we can also use digitize to binarize, having only one bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Grouping observations using clustering(Numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to cluster observations so that similar observations are grouped together. If we know, we have k groups, we can use k-means clustering to group similar observations and output a new feature containing each observation's group membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9.877554</td>\n",
       "      <td>-3.336145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.287210</td>\n",
       "      <td>-8.353986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.943061</td>\n",
       "      <td>-7.023744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.440167</td>\n",
       "      <td>-8.791959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.641388</td>\n",
       "      <td>-8.075888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  group\n",
       "0  -9.877554  -3.336145      2\n",
       "1  -7.287210  -8.353986      0\n",
       "2  -6.943061  -7.023744      0\n",
       "3  -7.440167  -8.791959      0\n",
       "4  -6.641388  -8.075888      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "features, _ = make_blobs(n_samples=50, n_features=2, centers=3, random_state=1)\n",
    "\n",
    "df = pd.DataFrame(features, columns=['feature_1', 'feature_2'])\n",
    "\n",
    "clusterer = KMeans(3, random_state=0)\n",
    "\n",
    "clusterer.fit(features)\n",
    "\n",
    "df['group'] = clusterer.predict(features)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Deleting Observations with Missing Values(Numerical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1, 11.1],\n",
       "       [ 2.2, 22.2],\n",
       "       [ 3.3, 33.3],\n",
       "       [ 4.4, 44.4]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = np.array([[1.1, 11.1], [2.2, 22.2], [3.3, 33.3], [4.4, 44.4], [np.nan, 55]])\n",
    "\n",
    "# Keep only observations that are not (denoted by -) missing\n",
    "new_features = features[~np.isnan(features).any(axis=1)]\n",
    "\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0        1.1       11.1\n",
       "1        2.2       22.2\n",
       "2        3.3       33.3\n",
       "3        4.4       44.4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternate way of dropping of mossing observations using Pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(features, columns=['feature_1', 'feature_2'])\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Encoding nominal categorical features(Categorical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a feature with nimonal classes that has no intrinsic ordering(e.g. apple, pear, banana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode the features using sklearn LabelBinarizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "\n",
    "feature = np.array([['Mumbai'], ['Bangalore'], ['New Delhi'], ['Kolkata'], ['Chennai']])\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "\n",
    "new_feature = one_hot.fit_transform(feature)\n",
    "new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bangalore', 'Chennai', 'Kolkata', 'Mumbai', 'New Delhi'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different classes\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mumbai', 'Bangalore', 'New Delhi', 'Kolkata', 'Chennai'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse transform\n",
    "one_hot.inverse_transform(new_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bangalore</th>\n",
       "      <th>Chennai</th>\n",
       "      <th>Kolkata</th>\n",
       "      <th>Mumbai</th>\n",
       "      <th>New Delhi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bangalore  Chennai  Kolkata  Mumbai  New Delhi\n",
       "0          0        0        0       1          0\n",
       "1          1        0        0       0          0\n",
       "2          0        0        0       0          1\n",
       "3          0        0        1       0          0\n",
       "4          0        1        0       0          0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternate way using Pandas\n",
    "import pandas as pd\n",
    "\n",
    "pd.get_dummies(feature[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Encoding nominal categorical features of multi-class(Categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_feature = [('Texas', 'Florida'), ('California', 'Alabama'), ('Texas', 'Florida'), ('Delware', 'Florida'), ('Texas', \"Alabama\")]\n",
    "\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "\n",
    "new_multiclass_feature = one_hot_multiclass.fit_transform(multiclass_feature)\n",
    "\n",
    "new_multiclass_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different classes\n",
    "one_hot_multiclass.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Encoding Ordinal categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal categorical features are ex:- high, medium, low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use of Pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Score' : ['Low', 'Low', 'Medium', 'Medium', 'High']})\n",
    "\n",
    "scale_mapper = {'Low':1, 'Medium':2, 'High':3}\n",
    "\n",
    "df = df['Score'].replace(scale_mapper)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    4\n",
       "5    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For more than 3 ordinal categorical features\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Score' : ['Low', 'Low', 'Medium', 'Medium', 'High', 'Barely more than medium']})\n",
    "\n",
    "scale_mapper = {'Low':1, 'Medium':2, 'Barely more than medium':3, 'High':4}\n",
    "\n",
    "df = df['Score'].replace(scale_mapper)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    4.0\n",
       "5    2.2\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For more than 3 ordinal categorical features\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Score' : ['Low', 'Low', 'Medium', 'Medium', 'High', 'Barely more than medium']})\n",
    "\n",
    "scale_mapper = {'Low':1, 'Medium':2, 'Barely more than medium':2.2, 'High':4}\n",
    "\n",
    "df = df['Score'].replace(scale_mapper)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Encoding dictionaries of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dicionary and want to convert into a feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data_dict = [{'Red':2,'Blue':4}, {'Red':4, 'Blue':3}, {'Red':1, 'Yellow':2},{'Red':2, 'Yellow':2}]\n",
    "\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "features = dict_vectorizer.fit_transform(data_dict)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Basic Cleaning of Given Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove whitespaces\n",
    "2. Remove periods '.'\n",
    "3. Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anskhoiuhc. By aoihouhoie HJkoiojueiujbh',\n",
       " 'Pakiuguiyvc iyubviygiy. uhiu',\n",
       " 'Ueiyvuc kaushiuhi asiuhi. aiugiuy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given text\n",
    "text_data = ['    Anskhoiuhc. By aoihouhoie HJkoiojueiujbh    ', \n",
    "             'Pakiuguiyvc iyubviygiy. uhiu', '     Ueiyvuc kaushiuhi asiuhi. aiugiuy    ']\n",
    "\n",
    "# Strip whitespaces\n",
    "strip_whitespace = [a.strip() for a in text_data]\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anskhoiuhc By aoihouhoie HJkoiojueiujbh',\n",
       " 'Pakiuguiyvc iyubviygiy uhiu',\n",
       " 'Ueiyvuc kaushiuhi asiuhi aiugiuy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove periods '.'\n",
    "remove_periods = [a.replace('.','') for a in strip_whitespace]\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anskhoiuhc by aoihouhoie hjkoiojueiujbh',\n",
       " 'pakiuguiyvc iyubviygiy uhiu',\n",
       " 'ueiyvuc kaushiuhi asiuhi aiugiuy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "lowercase = [a.lower() for a in remove_periods]\n",
    "lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Parsing and Cleaning HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have text data with HTML elements and want to extract just the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal et Communication\n",
      "Ingénierie Réseaux et Télécommunications\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = '''\n",
    "<td><a href=\"http://www.irit.fr/SC\">Signal et Communication</a>\n",
    "<br/><a href=\"http://www.irit.fr/IRT\">Ingénierie Réseaux et Télécommunications</a>\n",
    "</td>\n",
    "'''\n",
    "soup = BeautifulSoup(text)\n",
    "\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to Machine learning for beginners'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punctuation marks \n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "text = \"Welcome???@@##$ to#$% Machine%$^ learning$%^& for beginners\"\n",
    "\n",
    "for x in text.lower():\n",
    "    if x in punctuations:\n",
    "        text = text.replace(x, '')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenizing into words\n",
    "2. Tokenizing into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have text and break into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'good', 'reference', 'for', 'machine', 'learning', 'operations']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is good reference for machine learning operations\"\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi how are you.',\n",
       " 'I am fine how about you.',\n",
       " 'Me too fine.',\n",
       " 'Nice to meet you.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hi how are you. I am fine how about you. Me too fine. Nice to meet you.\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given text data remove extremely common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'good', 'reference', 'machine', 'learning', 'operations']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is good reference for machine learning operations\"\n",
    "text = word_tokenize(text)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "cleaned_text = [word for word in text if word not in stop_words]\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to see first 10 stopwords\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming :\n",
      "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']\n",
      "\n",
      "\n",
      "After stemming :\n",
      "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "print('Before stemming :')\n",
    "print(stemmed_words)\n",
    "print('\\n')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed_words = [porter.stem(word) for word in tokenized_words]\n",
    "print('After stemming :')\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Encoding Text as Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have text data and want to create a set of features indicating the number of times an observation's text contains a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['Machine learning', 'Deep learning', 'Artificial Intelligence'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = bag_of_words.toarray()\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial', 'deep', 'intelligence', 'learning', 'machine']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting feature names\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine': 4, 'learning': 3, 'deep': 1, 'artificial': 0, 'intelligence': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Encoding Text with Weighing word importance(TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['Machine learning', 'Deep learning', 'Artificial Intelligence'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.60534851, 0.79596054],\n",
       "       [0.        , 0.79596054, 0.        , 0.60534851, 0.        ],\n",
       "       [0.70710678, 0.        , 0.70710678, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial', 'deep', 'intelligence', 'learning', 'machine']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine': 4, 'learning': 3, 'deep': 1, 'artificial': 0, 'intelligence': 2}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Reducing Features using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given set of features reduce the number of features while retaining the variance of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.33501649, -0.04308102, ..., -1.14664746,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -1.09493684, ...,  0.54856067,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -1.09493684, ...,  1.56568555,\n",
       "         1.6951369 , -0.19600752],\n",
       "       ...,\n",
       "       [ 0.        , -0.33501649, -0.88456568, ..., -0.12952258,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649, -0.67419451, ...,  0.8876023 ,\n",
       "        -0.5056698 , -0.19600752],\n",
       "       [ 0.        , -0.33501649,  1.00877481, ...,  0.8876023 ,\n",
       "        -0.26113572, -0.19600752]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70631939, -0.39512814, -1.73816236, ...,  0.36526417,\n",
       "        -0.31369006,  0.05355504],\n",
       "       [ 0.21732591,  0.38276482,  1.72878893, ..., -0.17818068,\n",
       "        -0.14031747,  1.18179755],\n",
       "       [ 0.4804351 , -0.13130437,  1.33172761, ..., -0.01924571,\n",
       "        -0.23580029,  0.92966158],\n",
       "       ...,\n",
       "       [ 0.37732433, -0.0612296 ,  1.0879821 , ..., -1.05526847,\n",
       "         1.75559618, -0.87894699],\n",
       "       [ 0.39705007, -0.15768102, -1.08160094, ...,  0.10442881,\n",
       "         0.65907949,  1.1292155 ],\n",
       "       [-0.46407544, -0.92213976,  0.12493334, ..., -1.10593026,\n",
       "         0.54434185, -0.26573597]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "\n",
    "features_pca = pca.fit_transform(features)\n",
    "features_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features : 64\n",
      "Reduced number of features : 54\n"
     ]
    }
   ],
   "source": [
    "print('Original number of features :', features.shape[1])\n",
    "print('Reduced number of features :', features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Thresholding Numerical Feature Variance (Dimensionality reduction technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of numerical features and want to remove those with low variance(ie. likely containing little information). Need to select subset of features with variances above given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=0.5)\n",
    "\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data matrix :\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n",
      "\n",
      "\n",
      "High variance data matrix :\n",
      " [[5.1 1.4 0.2]\n",
      " [4.9 1.4 0.2]\n",
      " [4.7 1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print('Original data matrix :\\n', features[0:3])\n",
    "print('\\n')\n",
    "print('High variance data matrix :\\n', features_high_variance[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variances of different features :\n",
      "\n",
      "[0.68112222 0.18871289 3.09550267 0.57713289]\n"
     ]
    }
   ],
   "source": [
    "print('Variances of different features :\\n')\n",
    "print(thresholder.variances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Thresholding binary feature variance(dimensionality reduction technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of binary categorical features and want to remove those with low variance(ie. likely containing little information). Select a subset of features with a Bernoulli random variable variance above a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "features = [[0, 1, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0]]\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=(0.75*(1-0.75))) \n",
    "\n",
    "thresholder.fit_transform(features)\n",
    "\n",
    "# note : In binary features ie. Bernoulli random variable variance is calculated as Var(x)=p(1-p)\n",
    "#        where p is the proportion of observations of class 1. Hence by setting p, we can remove\n",
    "#        features where the vast majority of observations are one class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Handling Highly Correlated features(dimensionality reduction technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a feature matrix and suspect some features are highly correlated. Using this correlation matrix to check highly correlated features. If highly correlated features exists, consider dropping one of the correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [2, 2, 0],\n",
       "       [3, 3, 1],\n",
       "       [4, 4, 0],\n",
       "       [5, 5, 1],\n",
       "       [6, 6, 0],\n",
       "       [7, 7, 1],\n",
       "       [8, 7, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[1, 1, 1], [2, 2, 0], [3, 3, 1], [4, 4, 0], [5, 5, 1], [6, 6, 0], [7, 7, 1], [8, 7, 0]])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  1  1  1\n",
       "1  2  2  0\n",
       "2  3  3  1\n",
       "3  4  4  0\n",
       "4  5  5  1\n",
       "5  6  6  0\n",
       "6  7  7  1\n",
       "7  8  7  0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert feature matrix into dataframe\n",
    "df = pd.DataFrame(features)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.218218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.177084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.991837  0.218218\n",
       "1  0.991837  1.000000  0.177084\n",
       "2  0.218218  0.177084  1.000000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>0.218218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.177084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2\n",
       "0 NaN  0.991837  0.218218\n",
       "1 NaN       NaN  0.177084\n",
       "2 NaN       NaN       NaN"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1\n",
       "3  4  0\n",
       "4  5  1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column]>0.95)]\n",
    "df.drop(df.columns[to_drop], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Removing Irrelavant Features for Classification(dimensionality reduction technique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have categorical target vector and want to remove uninformative features.\n",
    "1. If the features are categorical, calculate chi-square statistic between each feature and the target vector.\n",
    "2. If the features are quantitative, compute the ANOVA F-value between each feature and the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original no. of features : 4\n",
      "Reduced no. of features : 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# convert to categorical data by converting data to integers\n",
    "features = features.astype(int)\n",
    "\n",
    "# select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "print('Original no. of features :', features.shape[1])\n",
    "print('Reduced no. of features :', features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original no. of features : 4\n",
      "Reduced no. of features : 2\n"
     ]
    }
   ],
   "source": [
    "# If the features are quantitative, compute the ANOVA F-value between each feature and the target vector\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "print('Original no. of features :', features.shape[1])\n",
    "print('Reduced no. of features :', features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Cross Validating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate how well our model will work in the real world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693916821849783"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# create feature matrix\n",
    "features = digits.data\n",
    "\n",
    "# create target vector\n",
    "target = digits.target\n",
    "\n",
    "# create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# create logistic regression object\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(standardizer, logistic)\n",
    "\n",
    "# create k-fild cv\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# conduct k-fold cv\n",
    "cv_results = cross_val_score(pipeline, features, target, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# calculate mean\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.98888889, 0.96111111, 0.94444444, 0.97777778,\n",
       "       0.98333333, 0.95555556, 0.98882682, 0.97765363, 0.93854749])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. Create a Baseline Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a simple baseline regression model to complare against your model. Sklearn's DummtRegressor helps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001119359203955339"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "# create features\n",
    "features, target = boston.data, boston.target\n",
    "\n",
    "# train test split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=0)\n",
    "\n",
    "# create a dummy regressor\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "\n",
    "# train dummy regressor\n",
    "dummy.fit(features_train, target_train)\n",
    "\n",
    "# get R-squared score\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6354638433202114"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compare, we train out model and evaluate the performance score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "ols = LinearRegression()\n",
    "ols.fit(features_train, target_train)\n",
    "\n",
    "# R-squared score\n",
    "ols.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. Create a Baseline Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42105263157894735"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_iris()\n",
    "\n",
    "# create features\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# train test split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=0)\n",
    "\n",
    "# create a dummy regressor\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# train dummy regressor\n",
    "dummy.fit(features_train, target_train)\n",
    "\n",
    "# get R-squared score\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compare, we train out model and evaluate the performance score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(features_train, target_train)\n",
    "\n",
    "# R-squared score\n",
    "classifier.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. Evaluating Binary Classifier Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9555, 0.95  , 0.9585, 0.9555, 0.956 ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=10000, n_features=3, n_informative=3, n_redundant=0, n_classes=2, random_state=1)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "cross_val_score(logistic, X, y, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95963673, 0.94820717, 0.9635996 , 0.96149949, 0.96060606])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logistic, X, y, scoring = 'precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.951, 0.952, 0.953, 0.949, 0.951])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logistic, X, y, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95529884, 0.9500998 , 0.95827049, 0.95520886, 0.95577889])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logistic, X, y, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using CV if we have true y values and the predicted y values. We can calculate metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "y_hat = logistic.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. Evaluating Binary Classifier Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate a binary classifier and various probability thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create feature matrix and target vector\n",
    "features, target = make_classification(n_samples=10000, n_features=10, n_classes=2, n_informative=3, random_state=3)\n",
    "\n",
    "# split into training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# create classifier\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# train model\n",
    "logistic.fit(features_train, target_train)\n",
    "\n",
    "# get predicted probabilities\n",
    "target_probabilities = logistic.predict_proba(features_test)[:,1]\n",
    "\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(target_test, target_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0], c=\".7\"), plt.plot([1, 1], c=\".7\")\n",
    "plt.ylabel(\"True +ve rate\")\n",
    "plt.xlabel(\"False +ve rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32. Evaluating Multicalss Classifier Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a model that predicts three or more classes and want to evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.841 , 0.829 , 0.8265, 0.8155, 0.82  ])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# generate feature matrix and target vector\n",
    "features, target = make_classification(n_samples=10000, n_features=3, n_informative=3, n_redundant=0, n_classes=3, random_state=1)\n",
    "\n",
    "# create logistic regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# cross-validate model using accuracy\n",
    "cross_val_score(logistic, features, target, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.841 , 0.829 , 0.8265, 0.8155, 0.82  ])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-validate model using accuracy and parallel processing\n",
    "cross_val_score(logistic, features, target, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.841 , 0.829 , 0.8265, 0.8155, 0.82  ])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-validate model using f1 micro\n",
    "cross_val_score(logistic, features, target, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33. Visualizing a Classifier's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given predicted classes and true classes of the test data, want to visually compare model's quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# create feature matrix\n",
    "features = iris.data\n",
    "\n",
    "# create target vector\n",
    "target = iris.target\n",
    "\n",
    "# create alist of target classnames\n",
    "class_names = iris.target_names\n",
    "\n",
    "# create training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=1)\n",
    "\n",
    "# create logostic regression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# train model and make predictions\n",
    "target_predicted = logistic.fit(features_train, target_train).predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 15,  1],\n",
       "       [ 0,  0,  9]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create confuction matrix\n",
    "matrix = confusion_matrix(target_test, target_predicted)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            setosa  versicolor  virginica\n",
       "setosa          13           0          0\n",
       "versicolor       0          15          1\n",
       "virginica        0           0          9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create pandas dataframe\n",
    "df = pd.DataFrame(matrix, index=class_names, columns=class_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create heatmap\n",
    "sns.heatmap(df, annot=True, cbar=None, cmap='Blues')\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 34. Evaluating Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate performance of the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1974.65337976, -2004.54137625, -3935.19355723, -1060.04361386,\n",
       "       -1598.74104702])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate feature matrix and target vector\n",
    "features, target = make_regression(n_samples=100, n_features=3, n_informative=3, n_targets=1, noise=50, coef=False, random_state=1)\n",
    "\n",
    "# create a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# cross validate linear regression using neagative MSE\n",
    "cross_val_score(lin_reg, features, target, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8622399 , 0.85838075, 0.74723548, 0.91354743, 0.84469331])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validate linear regression using neagative R2\n",
    "cross_val_score(lin_reg, features, target, scoring='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note :</b><br>\n",
    "    1> Negative MSE best value = 0.0<br>\n",
    "    2> R2 best values : 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35. Selecting Best Model using Exhaustive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select the best model by searching over a range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "targets = iris.target\n",
    "\n",
    "# create logistic regression model\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "\n",
    "# create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# create range of candidate regularization hyperparameter values\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "#create dictionary hyperparameter values\n",
    "hyperparams = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create grid search\n",
    "gridsearch = GridSearchCV(log_reg, hyperparams, cv=5, verbose=0)\n",
    "\n",
    "#fit grid search\n",
    "best_model = gridsearch.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty: l2\n",
      "Best C: 7.742636826811269\n"
     ]
    }
   ],
   "source": [
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict target vector\\\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 36. Selecting Best Model using Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "targets = iris.target\n",
    "\n",
    "# create logistic regression model\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "\n",
    "# create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# create distibution of candidate regularization hyperparameter values\n",
    "C = uniform(loc=0, scale=4)\n",
    "\n",
    "#create dictionary hyperparameter values\n",
    "hyperparams = dict(C=C, penalty=penalty)\n",
    "\n",
    "# create grid search\n",
    "randomsearch = RandomizedSearchCV(log_reg, hyperparams, n_iter=100, cv=5, verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty: l2\n",
      "Best C: 2.4381112763728496\n"
     ]
    }
   ],
   "source": [
    "best_model = randomsearch.fit(features, targets)\n",
    "\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37. Linear Regression - Fitting Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a model that represents a linear relationship between feature and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load data with only 2 features\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "# create linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "model = lin_reg.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.485628113468223"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the intercept\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35207832,  0.11610909])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the feature coefficients\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38. Logistic Regression - Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train a simple binary classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with only 2 classes\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:]\n",
    "target = iris.target[:100]\n",
    "\n",
    "# standardize feature\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "# train model\n",
    "model = log_reg.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new observation\n",
    "new_observation = [[.5, .5, .5, .5]]\n",
    "\n",
    "# predict class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17738424, 0.82261576]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view predicted probabilities\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39. Logistic Regression - Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given more than 2 classes, need to train a classifier model. Logistic regression model using one-vs-rest or multinomial methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# standardize feature\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "log_reg = LogisticRegression(random_state=0, multi_class='ovr')\n",
    "\n",
    "# train model\n",
    "model = log_reg.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new observation\n",
    "new_observation = [[.5, .5, .5, .5]]\n",
    "\n",
    "# predict class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.02652534e-01, 4.97344952e-01, 2.51345605e-06]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view predicted probabilities\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. Logistic Regression - Reducing Variance through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reduce variance of the logistic regression model using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# standardize feature\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# create logistic regression classifier object\n",
    "log_reg = LogisticRegressionCV(penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
    "\n",
    "# train model\n",
    "model = log_reg.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new observation\n",
    "new_observation = [[.5, .5, .5, .5]]\n",
    "\n",
    "# predict class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.96244929e-04, 9.70140320e-01, 2.92634349e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view predicted probabilities\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 41. Training a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train a classifier using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# create decision tree classifier object\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# train model\n",
    "model.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new observation\n",
    "observation = [[5, 4, 3, 2]]\n",
    "\n",
    "# predict observation's class\n",
    "model.predict(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted class probabilities\n",
    "model.predict_proba(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted : [1]\n",
      "Predicted class probabilities :  [[0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Training model with 'entropy'\n",
    "model_with_entropy = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "\n",
    "model_with_entropy.fit(features, target)\n",
    "\n",
    "# new observation\n",
    "observation = [[5, 4, 3, 2]]\n",
    "\n",
    "# predict observation's class\n",
    "print('Predicted :', model.predict(observation))\n",
    "\n",
    "# predicted class probabilities\n",
    "print('Predicted class probabilities : ',model.predict_proba(observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. Training Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=0, splitter='best')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data with only 2 features\n",
    "boston = datasets.load_boston()\n",
    "features = boston.data[:, 0:2]\n",
    "target = boston.target\n",
    "\n",
    "# create decision tree regressor object\n",
    "model = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# train model\n",
    "model.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction :  [33.]\n"
     ]
    }
   ],
   "source": [
    "# new observation\n",
    "observation = [[0.02, 16]]\n",
    "\n",
    "# predict model\n",
    "print('Model prediction : ',model.predict(observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with creterion as 'mae' ie. mean absolute error\n",
    "new_model = DecisionTreeRegressor(random_state=0)\n",
    "new_model.fit(features, target)\n",
    "observation = [[0.02, 160]]\n",
    "model.predict(observation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
